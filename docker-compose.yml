version: "3.7"

services:
  zookeeper:  
    hostname: zookeeper
    container_name: zookeeper
    image: 'bitnami/zookeeper:latest'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes


  nifi:
    image: apache/nifi:latest
    ports:
      - 8080:8080 
    environment:
      - NIFI_WEB_HTTP_PORT=8080
      - NIFI_CLUSTER_IS_NODE=true
      - NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
      - NIFI_ZK_CONNECT_STRING=zookeeper:2181
      - NIFI_ELECTION_MAX_WAIT=10 sec
      - NIFI_ANALYTICS_QUERY_INTERVAL=-30 mins
    volumes:
      - ./resources/configs/nifi/conf:/opt/nifi/nifi-current/conf
    depends_on:
      - spark
      - minio-s3
  

  minio-s3:
    image: minio/minio
    ports:
      - "9000:9000"
    volumes:
      - minio_storage:/data
    environment:
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
    command: server --address 0.0.0.0:9000 /data    


  create-minio-buckets:
    image: minio/mc
    depends_on:
      - minio-s3
    restart: on-failure      
    volumes:
      - ./resources/tools:/tools
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc config host add localminio http://minio-s3:9000 minioadmin minioadmin &&
      /usr/bin/mc mb --ignore-existing localminio/raw-data &&
      /usr/bin/mc policy set download localminio/raw-data &&
      /usr/bin/mc mb --ignore-existing localminio/trusted-data &&
      /usr/bin/mc policy set download localminio/trusted-data &&
      exit 0;
      "    

  spark:
    image: jupyter/pyspark-notebook:spark-2
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - GRANT_SUDO=yes
      - PYSPARK_SUBMIT_ARGS= --packages "org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.7.4" pyspark-shell 
     #- PYSPARK_SUBMIT_ARGS= --packages "org.apache.hadoop:hadoop-aws:3.2.0" pyspark-shell 
     #- SPARK_OPTS=--driver-java-options=-Xms1024M --driver-java-options=-Xmx16G --driver-java-options=-Dlog4j.logLevel=info
    ports:
      - "8888:8888"
      - "8998:8998"
      - "4040-4080:4040-4080"
    user: root
    volumes:
      - $PWD:/home/jovyan/datalake-case
    command: >
      /bin/bash -c "
      echo 'spark.jars.packages org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.7.4' >> /usr/local/spark/conf/spark-defaults.conf &&
      echo 'spark.driver.memory 16g' >> /usr/local/spark/conf/spark-defaults.conf &&
      echo 'spark.master local[*]' >> /usr/local/spark/conf/spark-defaults.conf &&
      jupyter labextension install @jupyterlab/toc --no-build && jupyter lab build --minimize=False && 
      pip3 install -e /home/jovyan/datalake-case/src/python/datalakecase &&
      su - jovyan &&
      echo 'Installing apache Livy ...' && 
      wget https://downloads.apache.org/incubator/livy/0.7.0-incubating/apache-livy-0.7.0-incubating-bin.zip -O apache-livy-0.7.0-incubating-bin.zip &&
      unzip -o apache-livy-0.7.0-incubating-bin.zip && rm apache-livy-0.7.0-incubating-bin.zip && cd apache-livy-0.7.0-incubating-bin && mkdir logs -p &&
      ./bin/livy-server start && cd ~/ && rm work -R &&
      start-notebook.sh --NotebookApp.token='' "
 

  presto:
    hostname: presto
    image: 'starburstdata/presto:0.203-e.0.1'
    container_name: presto
    ports:
      - '18080:8080'
    volumes: 
      - ./resources/configs/presto/catalog/minio.properties:/usr/lib/presto/etc/catalog/minio.properties
      - ./resources/jars/json-serde-1.3.8-jar-with-dependencies.jar:/usr/lib/presto/lib/plugin/hive-hadoop2/json-serde-1.3.8-jar-with-dependencies.jar


  hadoop-hive:
    hostname: hadoop-master
    image: 'prestodb/cdh5.13-hive:latest'
    container_name: hadoop-master
    volumes:
      - ./resources/configs/hadoop/cdh5.13-hive/conf/core-site.xml:/etc/hadoop/conf/core-site.xml
      - ./resources/jars/json-udf-1.3.8-jar-with-dependencies.jar:/usr/lib/hive/lib/json-udf-1.3.8-jar-with-dependencies.jar
      - ./resources/jars/json-serde-1.3.8-jar-with-dependencies.jar:/usr/lib/hive/lib/json-serde-1.3.8-jar-with-dependencies.jar


  metabase:
    image: metabase/metabase
    restart: always
    ports:
      - 3000:3000
    volumes:
      - ./resources/configs/metabase/metabase-data:/metabase-data
    environment:
      MB_DB_FILE: /metabase-data/metabase.db
      JAVA_TOOL_OPTIONS: -Xmx1g


  check-services:
    image: ubuntu
    container_name: check-services
    environment:
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ./resources/tools:/tools
    restart: on-failure      
    user: root
    command: >
      /bin/bash -c "
      chmod +x /tools/* && /tools/check-services.sh
      exit 0
       "
      
  # ranger-db:
  #     image: spydernaz/apache-ranger-admin-db:latest
  #     restart: always
  #     environment:
  #         - MYSQL_ROOT_PASSWORD=admin
  #     depends_on:
  #         - "atlas"

  # apache-ranger:
  #     image: spydernaz/apache-ranger-admin:experimental
  #     depends_on:
  #         - "ranger-db"
  #     ports:
  #         - 6080:6080
  # atlas:
  #     image: spydernaz/apache-atlas:experimental
  #     ports:
  #         - 21000:21000

  # kafka:
  #     image: 'confluentinc/cp-kafka:5.0.0'
  #     environment:
  #         - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
  #         - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://:9092
  #         - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
  #         - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  #         - ALLOW_PLAINTEXT_LISTENER=yes
  #     depends_on:
  #         - zookeeper

volumes:
  nifi_flow:
  minio_storage:
  metabase_postgres_data:
